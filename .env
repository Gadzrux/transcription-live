# ──────────────────────────────────────────────────────────────
# Transcription Live – Environment Configuration
# ──────────────────────────────────────────────────────────────

# Whisper model to use. Options (best → lightest):
#   large-v3          ~1.5B params – best quality, ~2GB VRAM with int8
#   large-v3-turbo    ~800M params – fast + accurate, ~2GB VRAM with float16
#   medium            ~769M params – good middle ground
#   small             ~244M params – fast, decent quality
#   base / tiny       for quick smoke tests only
WHISPER_MODEL=large-v3

# Device for inference: "cuda" (GPU), "cpu", or "auto"
WHISPER_DEVICE=cuda

# Compute/quantization type. For GTX 1650 (4GB VRAM):
#   int8        – lowest VRAM, runs large-v3 comfortably (~1.5–2GB)
#   float16     – better for large-v3-turbo or medium
#   int8_float16 – hybrid, good balance
#   auto        – let faster-whisper pick
WHISPER_COMPUTE=int8

# Restrict language detection to these languages only (comma-separated ISO codes).
# Set to "auto" or leave empty to allow all languages.
# "hi,en" = only Hindi and English (prevents Urdu/Punjabi false detections)
WHISPER_LANGUAGES=hi,en

# ──────────────────────────────────────────────────────────────
# Latency tuning
# ──────────────────────────────────────────────────────────────

# Minimum gap (seconds) between successive transcriptions.
# Lower = more responsive, but GPU must keep up.
# GTX 1650 + large-v3 int8: ~1.5s is realistic.
# Bigger GPU or smaller model: try 1.0 or even 0.5.
TRANSCRIBE_INTERVAL=1.5

# Sliding window size (seconds). Only the last N seconds of audio
# are transcribed each cycle. Keeps latency constant over time.
# 10-15s is a good range. Shorter = faster but less context.
SLIDING_WINDOW=15
